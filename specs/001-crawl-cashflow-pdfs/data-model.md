# Data Model: Crawl Political Cash Flow PDFs

**Feature**: 001-crawl-cashflow-pdfs  
**Date**: 2026-01-19  
**Status**: Complete

## Overview

This document defines the core data entities and their relationships for the PDF crawling system. All models use Pydantic for validation and type safety.

## Core Entities

### 1. WebsiteConfiguration

Represents a government website to be crawled, containing the base URL and metadata.

**Fields**:
- `id` (str): Unique identifier for the website (e.g., "miac-national", "tokyo-prefecture")
- `name` (str): Human-readable name (e.g., "Ministry of Internal Affairs and Communications")
- `base_url` (str): Base URL of the website (must be valid HTTP/HTTPS URL)
- `description` (Optional[str]): Optional description of the website
- `crawl_frequency` (Optional[str]): Suggested crawl frequency (e.g., "monthly", "quarterly") - informational only
- `enabled` (bool): Whether this website should be crawled (default: True)

**Validation Rules**:
- `base_url` must be a valid HTTP/HTTPS URL (no file:// or other protocols)
- `id` must be unique within a configuration file
- `id` must be URL-safe (lowercase, alphanumeric, hyphens only)

**Example**:
```json
{
  "id": "miac-national",
  "name": "Ministry of Internal Affairs and Communications",
  "base_url": "https://www.soumu.go.jp/senkyo/seiji_s/",
  "description": "National political finance disclosure site",
  "crawl_frequency": "monthly",
  "enabled": true
}
```

### 2. WebsiteConfigurationList

Root configuration object containing the list of all websites to crawl.

**Fields**:
- `version` (str): Configuration schema version (e.g., "1.0")
- `websites` (List[WebsiteConfiguration]): List of website configurations

**Validation Rules**:
- Must contain at least one website
- All website IDs must be unique
- Version must follow semantic versioning format

**Example**:
```json
{
  "version": "1.0",
  "websites": [
    { "id": "miac-national", "name": "...", "base_url": "..." },
    { "id": "tokyo-prefecture", "name": "...", "base_url": "..." }
  ]
}
```

### 3. ScrapingStrategy

Represents the AI-determined approach for extracting PDFs from a specific website.

**Fields**:
- `website_id` (str): ID of the website this strategy applies to
- `strategy_type` (str): Type of strategy ("css_selector", "xpath", "regex", "pagination")
- `pdf_link_selectors` (List[str]): CSS selectors or XPath expressions to find PDF links
- `pagination_selector` (Optional[str]): Selector for "next page" link if pagination exists
- `max_pages` (Optional[int]): Maximum number of pages to crawl (default: 10)
- `metadata_extraction` (Optional[Dict[str, str]]): Optional selectors for extracting metadata (organization, period) from page
- `confidence` (float): AI confidence score (0.0-1.0) in this strategy

**Validation Rules**:
- `confidence` must be between 0.0 and 1.0
- `pdf_link_selectors` must not be empty
- `strategy_type` must be one of the defined types
- `max_pages` must be positive if provided

**State Transitions**:
- Created: Strategy generated by AI analysis
- Validated: Strategy tested against sample page
- Active: Strategy being used for crawling
- Failed: Strategy failed to extract PDFs, fallback used

### 4. DownloadedPDF

Represents a political cash flow report PDF file that has been downloaded.

**Fields**:
- `file_id` (str): Unique identifier for this file (UUID)
- `original_url` (str): Source URL where PDF was found
- `local_path` (str): Relative path where file is stored
- `filename` (str): Structured filename: `{organization-slug}_{reporting-period}_{original-name}.pdf`
- `file_size_bytes` (int): Size of the downloaded file in bytes
- `sha256_hash` (str): SHA-256 hash of file content for deduplication
- `organization_name` (Optional[str]): Extracted political party/organization name
- `organization_slug` (str): URL-safe organization identifier
- `reporting_period` (Optional[str]): Fiscal period (e.g., "2024-Q1", "2024-annual")
- `download_timestamp` (datetime): When the file was downloaded (ISO 8601 format)
- `website_id` (str): ID of source website configuration
- `http_status_code` (int): HTTP response status code
- `http_headers` (Dict[str, str]): Relevant HTTP response headers (Content-Type, Last-Modified, etc.)
- `crawl_status` (str): Status of crawl ("success", "failed", "partial")
- `error_message` (Optional[str]): Error message if crawl failed
- `metadata_version` (str): Version of metadata schema (for future migrations)

**Validation Rules**:
- `file_id` must be a valid UUID
- `original_url` must be a valid HTTP/HTTPS URL
- `sha256_hash` must be 64 hexadecimal characters
- `file_size_bytes` must be positive
- `crawl_status` must be one of: "success", "failed", "partial"
- `download_timestamp` must be valid ISO 8601 datetime
- `filename` must follow naming convention pattern
- `organization_slug` must be URL-safe

**State Transitions**:
1. **Discovered**: PDF URL found on website
2. **Downloading**: Download in progress
3. **Downloaded**: File successfully downloaded
4. **Validated**: PDF file validated as valid
5. **Stored**: File written to storage (local or Azure)
6. **Indexed**: Metadata recorded in metadata JSON
7. **Failed**: Download or validation failed

**Example**:
```json
{
  "file_id": "550e8400-e29b-41d4-a716-446655440000",
  "original_url": "https://www.soumu.go.jp/senkyo/seiji_s/reports/2024/ldp_2024_q1.pdf",
  "local_path": "pdfs/ldp_2024-q1_annual-report.pdf",
  "filename": "ldp_2024-q1_annual-report.pdf",
  "file_size_bytes": 2457600,
  "sha256_hash": "a3b2c1...",
  "organization_name": "Liberal Democratic Party",
  "organization_slug": "ldp",
  "reporting_period": "2024-Q1",
  "download_timestamp": "2026-01-19T10:30:00Z",
  "website_id": "miac-national",
  "http_status_code": 200,
  "http_headers": {
    "Content-Type": "application/pdf",
    "Last-Modified": "2024-04-15T08:00:00Z"
  },
  "crawl_status": "success",
  "error_message": null,
  "metadata_version": "1.0"
}
```

### 5. CrawlMetadata

Represents the collection-level information about a crawl execution.

**Fields**:
- `crawl_id` (str): Unique identifier for this crawl execution (UUID)
- `crawl_start_time` (datetime): When the crawl started (ISO 8601)
- `crawl_end_time` (Optional[datetime]): When the crawl completed (ISO 8601), None if in progress
- `total_websites` (int): Total number of websites in configuration
- `websites_crawled` (int): Number of websites successfully crawled
- `websites_failed` (int): Number of websites that failed to crawl
- `total_pdfs_discovered` (int): Total number of PDF URLs found
- `total_pdfs_downloaded` (int): Number of PDFs successfully downloaded
- `total_pdfs_failed` (int): Number of PDFs that failed to download
- `total_bytes_downloaded` (int): Total size of all downloaded files
- `parallelism` (int): Parallelism level used for this crawl
- `max_files_limit` (Optional[int]): Max files limit if specified, None for unlimited
- `output_folder` (str): Path to output folder (local or Azure)
- `storage_backend` (str): Type of storage used ("local" or "adls")
- `config_file_path` (str): Path to configuration file used
- `downloaded_files` (List[DownloadedPDF]): List of all downloaded PDF metadata
- `errors` (List[Dict[str, Any]]): List of errors encountered during crawl
- `metadata_version` (str): Version of metadata schema

**Validation Rules**:
- `crawl_id` must be a valid UUID
- `parallelism` must be positive
- `websites_crawled + websites_failed` <= `total_websites`
- `total_pdfs_downloaded + total_pdfs_failed` <= `total_pdfs_discovered`
- `crawl_end_time` must be after `crawl_start_time` if present
- `storage_backend` must be one of: "local", "adls"
- `total_bytes_downloaded` must be non-negative

**Example**:
```json
{
  "crawl_id": "660e9500-f39c-41d4-b826-556655440000",
  "crawl_start_time": "2026-01-19T10:00:00Z",
  "crawl_end_time": "2026-01-19T10:25:00Z",
  "total_websites": 10,
  "websites_crawled": 9,
  "websites_failed": 1,
  "total_pdfs_discovered": 47,
  "total_pdfs_downloaded": 45,
  "total_pdfs_failed": 2,
  "total_bytes_downloaded": 125829120,
  "parallelism": 5,
  "max_files_limit": null,
  "output_folder": "/path/to/output",
  "storage_backend": "local",
  "config_file_path": "/path/to/config.json",
  "downloaded_files": [ /* list of DownloadedPDF objects */ ],
  "errors": [
    {
      "website_id": "site-xyz",
      "error_type": "NetworkError",
      "error_message": "Connection timeout",
      "timestamp": "2026-01-19T10:15:00Z"
    }
  ],
  "metadata_version": "1.0"
}
```

## Relationships

```
WebsiteConfigurationList
  └── contains many → WebsiteConfiguration
                           ↓
                      (crawled using)
                           ↓
                    ScrapingStrategy
                           ↓
                      (produces)
                           ↓
                     DownloadedPDF
                           ↓
                      (tracked in)
                           ↓
                    CrawlMetadata
```

## File System Structure

### Output Folder Layout

```
{output_folder}/
├── metadata.json                    # CrawlMetadata as JSON
└── pdfs/
    ├── ldp_2024-q1_report.pdf
    ├── cdp_2024-q1_report.pdf
    └── ...
```

### Metadata JSON Format

The `metadata.json` file contains a single `CrawlMetadata` object with all downloaded files and errors. It is updated atomically after each successful download to ensure consistency.

## Type Definitions (Python)

```python
from pydantic import BaseModel, Field, HttpUrl, field_validator
from typing import List, Optional, Dict, Any
from datetime import datetime
from uuid import UUID

class WebsiteConfiguration(BaseModel):
    id: str = Field(..., pattern=r"^[a-z0-9-]+$")
    name: str
    base_url: HttpUrl
    description: Optional[str] = None
    crawl_frequency: Optional[str] = None
    enabled: bool = True

class WebsiteConfigurationList(BaseModel):
    version: str = Field(..., pattern=r"^\d+\.\d+$")
    websites: List[WebsiteConfiguration]
    
    @field_validator("websites")
    def unique_ids(cls, v):
        ids = [w.id for w in v]
        if len(ids) != len(set(ids)):
            raise ValueError("Website IDs must be unique")
        return v

class ScrapingStrategy(BaseModel):
    website_id: str
    strategy_type: str = Field(..., pattern=r"^(css_selector|xpath|regex|pagination)$")
    pdf_link_selectors: List[str]
    pagination_selector: Optional[str] = None
    max_pages: Optional[int] = Field(default=10, gt=0)
    metadata_extraction: Optional[Dict[str, str]] = None
    confidence: float = Field(..., ge=0.0, le=1.0)

class DownloadedPDF(BaseModel):
    file_id: str
    original_url: HttpUrl
    local_path: str
    filename: str
    file_size_bytes: int = Field(..., gt=0)
    sha256_hash: str = Field(..., pattern=r"^[a-f0-9]{64}$")
    organization_name: Optional[str] = None
    organization_slug: str = Field(..., pattern=r"^[a-z0-9-]+$")
    reporting_period: Optional[str] = None
    download_timestamp: datetime
    website_id: str
    http_status_code: int
    http_headers: Dict[str, str]
    crawl_status: str = Field(..., pattern=r"^(success|failed|partial)$")
    error_message: Optional[str] = None
    metadata_version: str

class CrawlMetadata(BaseModel):
    crawl_id: str
    crawl_start_time: datetime
    crawl_end_time: Optional[datetime] = None
    total_websites: int = Field(..., ge=0)
    websites_crawled: int = Field(..., ge=0)
    websites_failed: int = Field(..., ge=0)
    total_pdfs_discovered: int = Field(..., ge=0)
    total_pdfs_downloaded: int = Field(..., ge=0)
    total_pdfs_failed: int = Field(..., ge=0)
    total_bytes_downloaded: int = Field(..., ge=0)
    parallelism: int = Field(..., gt=0)
    max_files_limit: Optional[int] = Field(default=None, gt=0)
    output_folder: str
    storage_backend: str = Field(..., pattern=r"^(local|adls)$")
    config_file_path: str
    downloaded_files: List[DownloadedPDF]
    errors: List[Dict[str, Any]]
    metadata_version: str
```

## Data Integrity and Constraints

### Deduplication Strategy
- Use `sha256_hash` to detect duplicate files
- Before downloading, check if hash already exists in metadata
- If duplicate found, skip download and log information
- Deduplication disabled by default in v1 (can be added in future iteration)

### Atomic Updates
- Metadata JSON file updated atomically using temporary file + rename
- Prevents corruption if process crashes during write
- File locking not required for v1 (single process, sequential metadata updates)

### Error Recovery
- Partial crawl state saved in metadata.json
- Can resume crawl by checking downloaded files list
- Resume functionality deferred to v2 (v1 always starts fresh)

## Migration Strategy

The `metadata_version` field in both `DownloadedPDF` and `CrawlMetadata` enables future schema migrations:

1. When schema changes, increment version (e.g., "1.0" → "1.1" or "2.0")
2. Implement migration functions to upgrade old metadata files
3. Always write new metadata with current version
4. Support reading older versions for backward compatibility

## Testing Considerations

### Unit Test Fixtures
- Sample `WebsiteConfiguration` objects for different government sites
- Mock `ScrapingStrategy` responses from AI
- Example `DownloadedPDF` objects with various states (success, failed)
- Sample `CrawlMetadata` for complete and partial crawls

### Validation Tests
- Invalid URLs in configuration
- Duplicate website IDs
- Invalid SHA-256 hashes
- Out-of-range confidence scores
- Negative file sizes
- Invalid crawl status values

### Edge Cases
- Empty website list
- Zero-byte PDF files
- Missing required fields
- Malformed timestamps
- Special characters in filenames
